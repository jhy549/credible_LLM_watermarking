{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianghaoyu/anaconda3/envs/kgw/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import watermarking.watermark_processor\n",
    "\n",
    "watermarking.watermark_processor = reload(watermarking.watermark_processor)\n",
    "from watermarking.watermark_processor import RepetitionPenaltyLogitsProcessor\n",
    "from transformers import LogitsProcessorList, MinLengthLogitsProcessor, LogitsProcessor\n",
    "from watermarking.utils.text_tools import truncate\n",
    "from watermarking.utils.load_local import load_local_model_or_tokenizer\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[proxychains] Strict chain  ...  127.0.0.1:8890  ...  127.0.0.1:8890  ...  huggingface.co:443  ...  OK\n",
      "[proxychains] Strict chain  ...  127.0.0.1:8890  ...  127.0.0.1:8890  ...  huggingface.co:443  ...  OK\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\")\n",
    "# tokenizer = load_local_model_or_tokenizer(\"facebook/opt-1.3b\", 'tokenizer')\n",
    "# model = load_local_model_or_tokenizer(\"facebook/opt-1.3b\", 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "lm_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "lm_model = AutoModelForCausalLM.from_pretrained('gpt2').to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "c4_sliced_and_filted = load_from_disk('./c4-train.00000-of-00512_sliced')\n",
    "c4_sliced_and_filted = c4_sliced_and_filted['train'].shuffle(seed=42).select(\n",
    "    range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select a prompt\n",
    "\n",
    "sample_idx = 98\n",
    "input_text = c4_sliced_and_filted[sample_idx]['text']\n",
    "tokenized_input = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
    "tokenized_input = truncate(tokenized_input, max_length=300)\n",
    "# # print(tokenized_input)\n",
    "# input_ids = tokenized_input['input_ids'].cpu().tolist()\n",
    "# print(len(input_ids[0]))\n",
    "# print(input_ids[0])\n",
    "# tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min_length_processor = MinLengthLogitsProcessor(min_length=1000,\n",
    "                                                eos_token_id=tokenizer.eos_token_id)\n",
    "repetition_processor = RepetitionPenaltyLogitsProcessor(penalty=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import watermarking.watermark_processors.message_models.lm_message_model\n",
    "watermarking.watermark_processors.message_models.lm_message_model = reload(watermarking.watermark_processors.message_models.lm_message_model)\n",
    "import watermarking.watermark_processors.message_model_processor\n",
    "watermarking.watermark_processors.message_model_processor = reload(watermarking.watermark_processors.message_model_processor)\n",
    "\n",
    "\n",
    "from watermarking.watermark_processors.message_models.lm_message_model import LMMessageModel\n",
    "from watermarking.watermark_processors.message_model_processor import WmProcessorMessageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lm_message_model = LMMessageModel(tokenizer=tokenizer,lm_model=model,lm_tokenizer=tokenizer,\n",
    "    delta = 1.1, lm_prefix_len=10, lm_topk=-1, message_code_len = 20,random_permutation_num=50)\n",
    "wm_precessor_message_model = WmProcessorMessageModel(message_model=lm_message_model,tokenizer=tokenizer,\n",
    "    encode_ratio=5,max_confidence_lbd=0.5,strategy='max_confidence', message=[42,54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianghaoyu/CTWL/watermarking/watermark_processors/message_model_processor.py:183: UserWarning: The reduce argument of torch.scatter with Tensor src is deprecated and will be removed in a future PyTorch release. Use torch.scatter_reduce instead for more reduction options. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:231.)\n",
      "  scores.scatter_(1, topk_indices, log_Ps, reduce='add')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " express their views during the primary elections.\n",
      "‘‘He (Chiyangwa) does not listen to us. He does not know what we want,\" he said.\n",
      "Dinha also accused Chiyangwa of favouring his close allies in the primary elections.\n",
      "The Zvimba South constituency is one of the strongholds of President Emmerson Mnangagwa.\n",
      "Chiyangwa, however, dismissed the allegations, saying they were aimed at tarnishing his image.\n",
      "‘‘I am confident that I will win this election. I am very sure of my victory,’’ he said.\n",
      "He added: ‘‘I have been in politics for more than 20 years. I have never seen anything like this in my life.\n",
      "‘‘This is just another attempt by the opposition to tarnish my image.''\n",
      "Chiyangwa said he would work hard to improve the lives of ordinary Zimbabweans\n"
     ]
    }
   ],
   "source": [
    "start_length = tokenized_input['input_ids'].shape[-1]\n",
    "wm_precessor_message_model.start_length = start_length\n",
    "output_tokens = model.generate(**tokenized_input, max_new_tokens=200, num_beams=4,\n",
    "                               logits_processor=LogitsProcessorList(\n",
    "                                   [min_length_processor, repetition_processor,\n",
    "                                    wm_precessor_message_model]))\n",
    "\n",
    "output_text = tokenizer.decode(output_tokens[0][tokenized_input['input_ids'].shape[-1]:],\n",
    "                               skip_special_tokens=True)\n",
    "print(output_text)\n",
    "prefix_and_output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "# print(prefix_and_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:01<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([42, 454698], (tensor([[1.1000, 0.0000, 1.1000,  ..., 0.0000, 1.1000, 1.1000],\n",
      "        [0.0000, 1.1000, 1.1000,  ..., 0.0000, 1.1000, 0.0000],\n",
      "        [1.1000, 1.1000, 1.1000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [1.1000, 1.1000, 1.1000,  ..., 1.1000, 1.1000, 1.1000],\n",
      "        [1.1000, 0.0000, 1.1000,  ..., 1.1000, 0.0000, 1.1000],\n",
      "        [1.1000, 1.1000, 1.1000,  ..., 1.1000, 0.0000, 1.1000]]), [(92, 2, 0.3561956286430359), (79, 0, 0.05510837957262993)]))\n"
     ]
    }
   ],
   "source": [
    "log_probs = wm_precessor_message_model.decode(output_text)\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 454698]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([42],\n",
       " (tensor([[1.1000, 0.0000, 1.1000,  ..., 0.0000, 1.1000, 1.1000],\n",
       "          [0.0000, 1.1000, 1.1000,  ..., 0.0000, 1.1000, 0.0000],\n",
       "          [1.1000, 1.1000, 1.1000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [1.1000, 1.1000, 1.1000,  ..., 1.1000, 1.1000, 1.1000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.1000, 1.1000],\n",
       "          [1.1000, 1.1000, 1.1000,  ..., 1.1000, 1.1000, 1.1000]]),\n",
       "  [(169, 5, 0.9507791996002197)]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import watermarking.watermark_processors.random_processor\n",
    "watermarking.watermark_processors.random_processor = reload(watermarking.watermark_processors.random_processor)\n",
    "\n",
    "from watermarking.watermark_processors.random_processor import WmProcessorRandom\n",
    "random_processor = WmProcessorRandom(message=[42,5,22,5465,34], tokenizer=tokenizer,delta=1.5,message_code_len = 20,\n",
    "     top_k=100, encode_ratio = 5.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m random_processor\u001b[38;5;241m.\u001b[39mstart_length \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_input\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m t_output_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenized_input, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                logits_processor\u001b[38;5;241m=\u001b[39mLogitsProcessorList(\n\u001b[1;32m      4\u001b[0m                                    [min_length_processor, repetition_processor,\n\u001b[1;32m      5\u001b[0m                                     random_processor]))\n\u001b[1;32m      6\u001b[0m t_output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(t_output_tokens[\u001b[38;5;241m0\u001b[39m][tokenized_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:],\n\u001b[1;32m      7\u001b[0m                                skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_input' is not defined"
     ]
    }
   ],
   "source": [
    "random_processor.start_length = tokenized_input['input_ids'].shape[-1]\n",
    "t_output_tokens = model.generate(**tokenized_input, max_new_tokens=200, num_beams=4,\n",
    "                               logits_processor=LogitsProcessorList(\n",
    "                                   [min_length_processor, repetition_processor,\n",
    "                                    random_processor]))\n",
    "t_output_text = tokenizer.decode(t_output_tokens[0][tokenized_input['input_ids'].shape[-1]:],\n",
    "                               skip_special_tokens=True)\n",
    "t_prefix_and_output_text = tokenizer.decode(t_output_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 2264.05it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 800.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t_log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_output_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTWL/watermarking/watermark_processors/random_processor.py:80\u001b[0m, in \u001b[0;36mWmProcessorRandom.decode\u001b[0;34m(self, text, messages, disable_tqdm)\u001b[0m\n\u001b[1;32m     78\u001b[0m     decoded_messages\u001b[38;5;241m.\u001b[39mappend(decoded_message)\n\u001b[1;32m     79\u001b[0m decoded_messages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(_) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m decoded_messages]\n\u001b[0;32m---> 80\u001b[0m cnts \u001b[38;5;241m=\u001b[39m \u001b[43mcnts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_messages, cnts\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 800.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "t_log_probs = random_processor.decode(t_output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 5]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_log_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "no_wm_output_tokens = model.generate(**tokenized_input, max_new_tokens=200, num_beams=4,\n",
    "                                     logits_processor=LogitsProcessorList(\n",
    "                                       [min_length_processor, repetition_processor]))\n",
    "no_wm_output_text = tokenizer.decode(no_wm_output_tokens[0][tokenized_input['input_ids'].shape[-1]:],\n",
    "                                     skip_special_tokens=True)\n",
    "no_wm_prefix_and_output_text = tokenizer.decode(no_wm_output_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianghaoyu/anaconda3/envs/kgw/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[proxychains] Strict chain  ...  127.0.0.1:8890  ...  127.0.0.1:8890  ...  cdn-lfs.huggingface.co:443  ...  OK\n"
     ]
    }
   ],
   "source": [
    "oracle_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-2.7b\")\n",
    "oracle_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-2.7b\")\n",
    "\n",
    "# oracle_tokenizer = load_local_model_or_tokenizer('facebook/opt-2.7b', 'tokenizer')\n",
    "# oracle_model = load_local_model_or_tokenizer('facebook/opt-2.7b', 'model')\n",
    "oracle_model = oracle_model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9879757165908813 2.6857922077178955\n"
     ]
    }
   ],
   "source": [
    "from watermarking.experiments.watermark import compute_ppl_single\n",
    "\n",
    "loss, ppl = compute_ppl_single(prefix_and_output_text=prefix_and_output_text,\n",
    "                               oracle_model_name='facebook/opt-2.7b',\n",
    "                               output_text=output_text,\n",
    "                               oracle_model=oracle_model, oracle_tokenizer=oracle_tokenizer)\n",
    "print(loss, ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0407838821411133 2.8314356803894043\n"
     ]
    }
   ],
   "source": [
    "from watermarking.experiments.watermark import compute_ppl_single\n",
    "\n",
    "loss, ppl = compute_ppl_single(prefix_and_output_text=t_prefix_and_output_text,\n",
    "                               oracle_model_name='facebook/opt-2.7b',\n",
    "                               output_text=t_output_text,\n",
    "                               oracle_model=oracle_model, oracle_tokenizer=oracle_tokenizer)\n",
    "print(loss, ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8734689950942993 2.395205497741699\n"
     ]
    }
   ],
   "source": [
    "loss, ppl = compute_ppl_single(prefix_and_output_text=no_wm_prefix_and_output_text,\n",
    "                               oracle_model_name='facebook/opt-2.7b',\n",
    "                               output_text=no_wm_output_text,\n",
    "                               oracle_model=oracle_model, oracle_tokenizer=oracle_tokenizer)\n",
    "print(loss, ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kgw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
